{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c90d71-311f-48b0-a596-978fe65f8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speech import *\n",
    "from classify import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f02af8f-58e9-48a4-a570-770ba0f983f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "def self_train(Xu, Xl, yl, C=0.7, confident_cutoff=0.8):\n",
    "    Xhat, yhat = Xl, yl\n",
    "    num_iter = 0\n",
    "    while True:\n",
    "        # Train\n",
    "        num_iter += 1\n",
    "        print(f\"{num_iter}th train\")\n",
    "        print(\"Data size:\", Xl.shape, yl.shape)\n",
    "        cls = LogisticRegression(max_iter=10000, n_jobs=-1, C=C)\n",
    "        cls.fit(Xhat, yhat)\n",
    "        \n",
    "        print(\"Evaluate Dev\")\n",
    "        evaluate(speech.devX, speech.devy, cls)\n",
    "\n",
    "        # Predict\n",
    "        print(\"Predicting unlabeled data with the previous model\")\n",
    "        yu_hat = cls.predict(Xu)\n",
    "        confidents = cls.predict_proba(Xu).max(axis=1)\n",
    "        print(confidents)\n",
    "\n",
    "        # Expand Confident samples\n",
    "        confident_Xu = Xu[confidents >= confident_cutoff]\n",
    "        confident_yu_hat = yu_hat[confidents >= confident_cutoff]\n",
    "        Xu = Xu[confidents < confident_cutoff]\n",
    "\n",
    "        if confident_Xu.shape[0] == 0:\n",
    "            print(\"Data size has converged\")\n",
    "            break\n",
    "            \n",
    "        Xhat = np.concatenate((Xl.toarray(), confident_Xu.toarray()), axis=0)\n",
    "        yhat = np.concatenate((yl, confident_yu_hat), axis=0)\n",
    "            \n",
    "    return cls, Xhat, yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b14c6-1fd6-498c-89cb-adf7b6fac409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE with training data:\n",
      "data/speech/train.tsv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class Lemmatizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, word):\n",
    "        return self.wnl.lemmatize(word)\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from speech import *\n",
    "from classify import evaluate\n",
    "\n",
    "def get_file_list(tsv_file):\n",
    "    print(tsv_file)\n",
    "    fnames = []\n",
    "    with open(tsv_file, 'r') as f:\n",
    "        for line in f:\n",
    "            fname, label = line.strip().split('\\t')\n",
    "            fnames.append(f\"data/speech/{fname}\")\n",
    "    return fnames\n",
    "\n",
    "class BPETokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "        print(\"Training BPE with training data:\")\n",
    "        files = get_file_list(\"data/speech/train.tsv\")\n",
    "        self.tokenizer.train(files=files, trainer=trainer)\n",
    "\n",
    "    def __call__(self, articles):\n",
    "        return self.tokenizer.encode(articles).tokens\n",
    "\n",
    "bpe_tokenizer = BPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257342b-7baf-4842-a7ed-f719140a0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading data\")\n",
    "tarfname = \"data/speech.tar.gz\"\n",
    "speech = read_files(tarfname, preprocessor=Lemmatizer(), tokenizer=bpe_tokenizer)\n",
    "print(speech.trainX.shape)\n",
    "\n",
    "print(\"Reading unlabeled data\")\n",
    "unlabeled = read_unlabeled(tarfname, speech)\n",
    "print(unlabeled.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fda4aa-bd4f-4273-a21f-8206707cc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training classifier\")\n",
    "cls, Xhat, yhat = self_train(unlabeled.X, speech.trainX, speech.trainy)\n",
    "\n",
    "print(\"Evaluating\")\n",
    "evaluate(speech.trainX, speech.trainy, cls)\n",
    "evaluate(speech.devX, speech.devy, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b7fa0-3814-4fca-91e7-f4ce7a664b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = np.unique(yhat)\n",
    "plt.hist(yhat, bins=bins-.5, edgecolor='black')\n",
    "plt.xticks(bins)\n",
    "plt.grid(axis='y')\n",
    "plt.title(\"Histogram of the training set labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbcb0bc-eba9-483d-b2ab-cd277b47a0cc",
   "metadata": {},
   "source": [
    "## Which labels to include in Dl_hat in every iteration?\n",
    "https://towardsdatascience.com/self-training-for-natural-language-understanding-d5c369b5a7f6\n",
    "\n",
    "Select top K samples from unlabeled data for each category based on the teacher's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045124d3-59ed-4f92-ac99-1349a228c35b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
