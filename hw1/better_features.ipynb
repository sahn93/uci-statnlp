{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "760d5720-96a6-4c10-9654-9bafd68b7d66",
   "metadata": {},
   "source": [
    "# Designing Better Features\n",
    "\n",
    "## The Sparsity of the Vocabulary\n",
    "\n",
    "- utilize the corpus of unlabeled text to learn something\n",
    "- knowledge from unlabeled documents can allow us to spread the labels to the words that do not even appear in the training data\n",
    "- e.g. co-occurence statistics on the unlabeled set of speeches\n",
    "\n",
    "### How to represent the word contexts?\n",
    "\n",
    "- word-document matrix\n",
    "- word-word matrix\n",
    "\n",
    "### How to compute similarity between word representations?\n",
    "\n",
    "- cosine distance\n",
    "- PMI(pairwise mutual information)\n",
    "\n",
    "### How to represent a document?\n",
    "\n",
    "- sum of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a6869f-7fd7-4ecd-9fa1-68b10b8a1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speech import *\n",
    "from classify import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d76d4721-d963-410b-9de6-2cd10f1c65a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE with training data:\n",
      "data/speech/train.tsv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class Lemmatizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, sentence):\n",
    "        return ' '.join([self.wnl.lemmatize(word) for word in sentence.split()])\n",
    "    \n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from speech import *\n",
    "from classify import evaluate\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "from os import listdir\n",
    "\n",
    "def get_file_list(tsv_file):\n",
    "    print(tsv_file)\n",
    "    fnames = []\n",
    "    with open(tsv_file, 'r') as f:\n",
    "        for line in f:\n",
    "            fname, label = line.strip().split('\\t')\n",
    "            fnames.append(f\"data/speech/{fname}\")\n",
    "    return fnames\n",
    "\n",
    "def get_unlabeled_file_list():\n",
    "    lst = []\n",
    "    dirname = 'data/speech/unlabeled'\n",
    "    for fname in listdir(dirname):\n",
    "        if \".txt\" in fname:\n",
    "            lst.append(f'{dirname}/{fname}')\n",
    "    return lst\n",
    "\n",
    "class BPETokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "        self.tokenizer.normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "        self.tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[CLS] $A [SEP]\",\n",
    "            pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "            special_tokens=[\n",
    "                (\"[CLS]\", 1),\n",
    "                (\"[SEP]\", 2),\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "        print(\"Training BPE with training data:\")\n",
    "        files = get_file_list(\"data/speech/train.tsv\") + get_unlabeled_file_list()\n",
    "        self.tokenizer.train(files=files, trainer=trainer)\n",
    "\n",
    "    def __call__(self, articles):\n",
    "        return self.tokenizer.encode(articles).tokens\n",
    "\n",
    "bpe_tokenizer = BPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "770e7f7e-a880-4c5a-89d7-0a58104dac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "def self_train(Xu, Xl, yl, devX, devy, C=0.7, confident_cutoff=0.8):\n",
    "    Xhat, yhat = Xl, yl\n",
    "    num_iter = 0\n",
    "    num_stall = 0\n",
    "    curr_best = .0\n",
    "    \n",
    "    clss = []\n",
    "    accs = []\n",
    "    data_sizes = []\n",
    "    \n",
    "    while True:\n",
    "        # Train\n",
    "        num_iter += 1\n",
    "        data_sizes.append(Xhat.shape[0])\n",
    "        print(f\"{num_iter}th train\")\n",
    "        print(\"Data size:\", Xhat.shape, yhat.shape)\n",
    "        cls = LogisticRegression(max_iter=10000, n_jobs=-1, C=C)\n",
    "        cls.fit(Xhat, yhat)\n",
    "        clss.append(cls)\n",
    "        \n",
    "        print(\"Evaluate Dev\")\n",
    "        acc = evaluate(devX, devy, cls)\n",
    "        accs.append(acc)\n",
    "        \n",
    "        if acc > curr_best:\n",
    "            print(\"new best score\")\n",
    "            curr_best = max(acc, curr_best)\n",
    "            num_stall = 0\n",
    "        else:\n",
    "            num_stall += 1\n",
    "            print(f\"stall {num_stall} times\")\n",
    "            if num_stall >= 5:\n",
    "                print(f\"dev accuracy is not improving for {num_stall} iterations. Stop.\")\n",
    "                break\n",
    "\n",
    "        # Predict\n",
    "        print(\"Predicting unlabeled data with the previous model\")\n",
    "        yu_hat = cls.predict(Xu)\n",
    "        confidents = cls.predict_proba(Xu).max(axis=1)\n",
    "\n",
    "        # Expand Confident samples\n",
    "        confident_Xu = Xu[confidents >= confident_cutoff]\n",
    "        confident_yu_hat = yu_hat[confidents >= confident_cutoff]\n",
    "        Xu = Xu[confidents < confident_cutoff]\n",
    "\n",
    "        if confident_Xu.shape[0] == 0:\n",
    "            print(\"Data size has converged\")\n",
    "            break\n",
    "        \n",
    "        print(\"Data added:\", confident_Xu.shape)\n",
    "        Xhat = vstack((Xhat, confident_Xu))\n",
    "        yhat = np.concatenate((yhat, confident_yu_hat), axis=0)\n",
    "            \n",
    "    return clss, accs, data_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9b28ade-e091-4bc7-a109-9b767c3c9edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "-- train data\n",
      "train.tsv\n",
      "4370\n",
      "-- dev data\n",
      "dev.tsv\n",
      "414\n",
      "-- transforming data and labels\n",
      "(4370, 59393)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "print(\"Reading data\")\n",
    "tarfname = \"data/speech.tar.gz\"\n",
    "speech = read_files(tarfname, preprocessor=Lemmatizer(), tokenizer=word_tokenize, ngram_range=(1,2))\n",
    "print(speech.trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5529553-cdd7-4e34-9009-f133e3760524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating\n",
      "  Accuracy 0.9995423340961098\n",
      "  Accuracy 0.41545893719806765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41545893719806765"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training classifier\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cls = LogisticRegression(max_iter=1000, n_jobs=-1, C=.5)\n",
    "cls.fit(speech.trainX, speech.trainy)\n",
    "\n",
    "print(\"Evaluating\")\n",
    "evaluate(speech.trainX, speech.trainy, cls)\n",
    "evaluate(speech.devX, speech.devy, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17862650-a662-49c5-b792-fab96369fc72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
